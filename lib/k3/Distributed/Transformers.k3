include "Annotation/Collection.k3"
include "Annotation/Map.k3"
include "Core/Barrier.k3"
include "Core/Builtins.k3"
include "Core/Messaging.k3"

control DistributedCollectionStatistics [ lbl         : label
                                        , query_cl    : [{cl:label, ce:expr}]
                                        , coordinator : expr
                                        , nodes       : expr
                                        , next        : expr ]
{
  ?e => $[lbl]_get_cstats(); $.[e]
     +> {
          // Statistics data structure populated at the coordinator.
          // Key is bucket address, value is bucket size.
          declare $[lbl]_cstats : mut collection {key: address, value: $[|
            -- This metaprogram creates a K3 record type using the list of collection labels.
            -- Each field is an integer type, to store the number of keys in the distributed collection.
            -- {<clbl 1> : int, <clbl 2> : int}
            let recErr _ = error "Invalid collection statistics type record" in
            let sizeFieldLT cRec = spliceRecordField cRec "cl" >>= idOfSLabel >>= \clbl -> return (clbl,TC.int)
            in maybe (recErr ()) (SType . TC.record) (elemsOfSList 'query_cl >>= mapM sizeFieldLT)
          |]} @Map

          // Use this method to gather collection statistics at the coordinator.
          declare $[lbl]_get_cstats : () -> () = \_ -> (
            $[nodes].iterate (\p -> ($[lbl]_collect_cstats, p.addr) <- ())
          )

          // Worker-side trigger to respond to statistics queries.
          trigger $[lbl]_collect_cstats : () = \_ -> (
            ($[lbl]_node_cstats, $[coordinator]) <- (me, $[|
              -- This metaprogram creates a record expression that contains fields
              -- populated with all of the desired collection sizes.
              let recErr _ = error "Invalid collection statistics record expr" in
              let sizeFieldLE cRec = do { l <- (spliceRecordField cRec "cl" >>= idOfSLabel);
                                          e <- (spliceRecordField cRec "ce" >>= expOfSExpr);
                                          return (l, EC.applyMany (EC.project "size" e) [EC.unit]) }
              in maybe (recErr ()) (SExpr . EC.record) (elemsOfSList 'query_cl >>= mapM sizeFieldLE)
            |])
          )

          // Coordinator-side trigger to accumulate collection statistics returns.
          // It prints the bucket distribution when all nodes are done reporting.
          trigger $[lbl]_node_cstats : (address, $[|
            -- This is the same K3 record constructor metaprogram as above.
            -- We could provide a common context of splice values in control annotation rewrites
            -- or support proper arbitrary nesting of Haskell/K3 metaprograms with a K3 quasiquoter.
            let recErr _ = error "Invalid collection statistics type record" in
            let sizeFieldLT cRec = spliceRecordField cRec "cl" >>= idOfSLabel >>= \clbl -> return (clbl,TC.int)
            in maybe (recErr ()) (SType . TC.record) (elemsOfSList 'query_cl >>= mapM sizeFieldLT)
          |])
          = \ns -> (
            (bind ns as (bucket_addr, cbucket_sizes) in
              $[lbl]_cstats.insert {key: bucket_addr, value: cbucket_sizes});
            ( print ($[|exprLabel 'lbl|] ++ " buckets: " ++
                      ($[lbl]_cstats.fold (\acc -> \r -> (acc ++ atos(r.key) ++ " => " ++ $[|
                      -- This metaprogram flattens all collection sizes into a K3 string expression.
                      let strError _ = error "Invalid collection statistics report" in
                      let sizeStr flbl = EC.applyMany (EC.variable "itos")
                                           [EC.project flbl $ EC.project "value" $ EC.variable "r"] in
                      let reportStr acc cRec = do { clbl <- spliceRecordField cRec "cl" >>= idOfSLabel;
                                                    return $ EC.binop OConcat acc
                                                           $ EC.binop OConcat (sizeStr clbl)
                                                           $ EC.constant (CString " ") }
                      in
                      let repExpr cRecs = foldM reportStr (EC.constant (CString "")) cRecs in
                      maybe (strError ()) SExpr $ (elemsOfSList 'query_cl >>= repExpr)
                      |]
                      )) ""));
              $[next]
            ) @OnCounter(id=[# $[lbl]_cstats_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )
        }
}


// Distributed Group By Implementation
// Enabling profiling will display the total elapsed time for the operator
// And skew statistics, namely:
//   - Difference in time between first and last peer finished with the group-by
//   - Number of keys assigned to each peer

control DistributedGroupByGeneric [ lbl             : label
                                  , peer_next       : expr
                                  , next            : expr
                                  , merge           : expr
                                  , coordinator     : expr
                                  , nodes           : expr
                                  , profile         : expr ]
{
  ignore ( (?e.groupBy ?groupF ?aggF ?accE) : collection ?t )
     => ((if $[profile]
          then $[lbl]_start_ts = now_int ()
          else ());
        $[nodes].iterate (\p -> ($[lbl]_scan, p.addr) <- ()) )
     +> {
          declare $[lbl]_start_ts : mut int = 0
          declare $[lbl]_end_ts : mut int = 0

          trigger $[lbl]_scan : () = \_ -> (
            (ignore ( $.[e].groupBy $.[groupF] $.[aggF] $.[accE]
            ) @SendPartitionByKey( dest_trg      = [$ $[lbl]_merge]
                                 , barrier_trg   = [$ $[lbl]_peer_barrier]
                                 , nodes         = nodes
                                 , send_extra_fn = [$ (\x -> x)]))
          )

          // Note: SendByPartitionKey always uses a @Collection for the wire type.
          trigger $[lbl]_merge : collection $::[t] @Collection = \partials -> (
            partials.iterate (\w -> $[merge] w)
          )

          trigger $[lbl]_peer_barrier : () = \_ -> (
            ( $[peer_next] ;
              ( $[lbl]_global_barrier, $[coordinator] ) <- ()
            ) @OnCounter(id=[# $[lbl]_peer_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )

          trigger $[lbl]_global_barrier : () = \_ -> (
            ( if $[profile]
                then ( $[lbl]_end_ts = now_int ();
                       let elapsed = $[lbl]_end_ts - $[lbl]_start_ts
                       in print ($[|exprLabel 'lbl|] ++ " total time: " ++ (itos elapsed))
                     )
                else ();
              $[next]
            ) @OnCounter(id=[# $[lbl]_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ $[profile]])
          )
        }
}

control DistributedGroupBy [ lbl             : label
                           , peer_next       : expr
                           , next            : expr
                           , merge           : expr
                           , coordinator     : expr
                           , nodes           : expr
                           , profile         : expr ]
{
  ignore ( (?e.groupBy ?groupF ?aggF ?accE) : collection ?t )
     => (ignore ($.[e].groupBy $.[groupF] $.[aggF] $.[accE]))
          @DistributedGroupByGeneric(
              lbl         = lbl
            , peer_next   = [$ $[peer_next] $[lbl]_result ]
            , next        = next
            , merge       = [$ (\v ->
                                  $[lbl]_result.insert_with v (\a -> \b ->
                                    {key:a.key, value: $[merge] a.value b.value}))
                            ]
            , coordinator = coordinator
            , nodes       = nodes
            , profile     = [$ $[profile] ] )

     +> { declare $[lbl]_result : mut collection $::[t] @Map }
}


control DistributedHashJoin [ lbl             : label
                            , lhs_query       : expr
                            , rhs_query       : expr
                            , lhs_build_merge : expr
                            , rhs_probe_merge : expr
                            , peer_next       : expr
                            , next            : expr
                            , coordinator     : expr
                            , nodes           : expr
                            , lhs_build_ty    : type
                            , rhs_probe_ty    : type
                            ]
{
  () => $[nodes].iterate (\p -> ($[lbl]_lhs_scan, p.addr) <- ())
     +> {
        trigger $[lbl]_lhs_scan : () = \_ -> (
          (ignore
            ( $[lhs_query]
            ) @SendPartitionByKey( dest_trg      = [$ $[lbl]_build_lhs]
                                 , barrier_trg   = [$ $[lbl]_lhs_peer_barrier]
                                 , nodes         = nodes
                                 , send_extra_fn = [$ (\x -> x)]))
        )

        trigger $[lbl]_build_lhs : $[lhs_build_ty] = \vals -> (
          vals.iterate $[lhs_build_merge]
        )

        trigger $[lbl]_lhs_peer_barrier : () = \_ -> (
          print "Hash join LHS finished build." ;
          ( $[nodes].iterate (\p -> ($[lbl]_rhs_scan, p.addr) <- ())
          ) @OnCounter(id=[# $[lbl]_peer_build_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_rhs_scan : () = \_ -> (
          (( ignore
              ( $[rhs_query]
              ) @SendPartitionByKey( dest_trg      = [$ $[lbl]_probe_rhs]
                                   , barrier_trg   = [$ $[lbl]_rhs_peer_barrier]
                                   , nodes         = nodes
                                   , send_extra_fn = [$ (\x -> x)]))
          ) @OnCounter(id=[# $[lbl]_lhs_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_probe_rhs : $[rhs_probe_ty] = \vals -> (
          // Probe LHS hash table.
          vals.iterate $[rhs_probe_merge]
        )

        trigger $[lbl]_rhs_peer_barrier : () = \_ -> (
          ( print "Hash join RHS finished probe." ;
            $[peer_next] ;
            ($[lbl]_global_barrier, $[coordinator]) <- ()
          ) @OnCounter(id=[# $[lbl]_rhs_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_global_barrier : () = \_ -> (
          ( print "Hash join done." ;
            $[next]
          ) @OnCounter(id=[# $[lbl]_join_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )
     }
}

control PipelineSymmetricHashJoin [ lbl                  : label
                                  , lhs_query            : expr
                                  , rhs_query            : expr
                                  , lhs_ht_ty            : type
                                  , rhs_ht_ty            : type
                                  , lhs_probe            : expr
                                  , rhs_probe            : expr
                                  , lhs_insert_with      : expr
                                  , rhs_insert_with      : expr
                                  , lhs_empty_out_buffer : expr
                                  , rhs_empty_out_buffer : expr
                                  , lhs_pipeline_next    : expr
                                  , rhs_pipeline_next    : expr
                                  , peer_next            : expr
                                  , next                 : expr
                                  , coordinator          : expr
                                  , nodes                : expr
                                  ]
{
  () => $[nodes].iterate (\p -> ($[lbl]_redistribute, p.addr) <- ())
     +> {
          declare $[lbl]_lhs_ht : collection $[lhs_ht_ty] @Map
          declare $[lbl]_rhs_ht : collection $[rhs_ht_ty] @Map
          declare $[lbl]_lhs_done : mut bool = false
          declare $[lbl]_rhs_done : mut bool = false

          trigger $[lbl]_redistribute : () = \_ -> (
            (ignore
              ( $[lhs_query]
              ) @SendPartitionByKey( dest_trg      = [$ $[lbl]_process_redistribute]
                                   , barrier_trg   = [$ $[lbl]_lhs_peer_barrier]
                                   , nodes         = nodes
                                   , send_extra_fn = [$ (\x -> (Some x, None immut, true))]));
            (ignore
              ( $[rhs_query]
              ) @SendPartitionByKey( dest_trg      = [$ $[lbl]_process_redistribute]
                                   , barrier_trg   = [$ $[lbl]_rhs_peer_barrier]
                                   , nodes         = nodes
                                   , send_extra_fn = [$ (\x -> (None immut, Some x, false))]))
          )

          trigger $[lbl]_process_redistribute : ( option (collection $[lhs_ht_ty] @Map)
                                                , option (collection $[rhs_ht_ty] @Map)
                                                , bool)
          = \rd -> (
            bind rd as (lv_opt, rv_opt, is_left) in (
              // Three stages: join other, pipeline outputs, track for other
              if is_left then (
                case lv_opt of
                { Some lvals ->
                  let outputs = lvals.fold $[rhs_probe] $[rhs_empty_out_buffer] in
                  $[lhs_pipeline_next] outputs;
                  if $[lbl]_rhs_done then () else ( lvals.iterate (\lkv -> $[lbl]_lhs_ht.insert_with lkv $[lhs_insert_with]) )
                }
                { None -> error ($[|exprLabel 'lbl|] ++ "_process_redistribute: invalid left join inputs.") }
              ) else (
                case rv_opt of
                { Some rvals ->
                  let outputs = rvals.fold $[lhs_probe] $[lhs_empty_out_buffer] in
                  $[rhs_pipeline_next] outputs;
                  if $[lbl]_lhs_done then () else ( rvals.iterate (\rkv -> $[lbl]_rhs_ht.insert_with rkv $[rhs_insert_with]) )
                }
                { None -> error ($[|exprLabel 'lbl|] ++ "_process_redistribute: invalid right join inputs.") }
              )
            )
          )

          trigger $[lbl]_lhs_peer_barrier : () = \_ -> (
            print "Hash join LHS finished processing." ;
            (
              // LHS is finished, we no longer need to build/probe the RHS
              $[lbl]_lhs_done = true;
              $[lbl]_rhs_ht.clear();
              if not $[lbl]_rhs_finished then () else (
                $[peer_next];
                ($[lbl]_global_barrier, $[coordinator]) <- ()
              )
            ) @OnCounter(id=[# $[lbl]_peer_lbuild_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )

          trigger $[lbl]_rhs_peer_barrier : () = \_ -> (
            print "Hash join RHS finished processing." ;
            (
              // RHS is finished, we no longer need to build/probe the LHS
              $[lbl]_rhs_done = true;
              $[lbl]_lhs_ht.clear();
              if not $[lbl]_lhs_finished then () else (
                $[peer_next];
                ($[lbl]_global_barrier, $[coordinator]) <- ()
              )
            ) @OnCounter(id=[# $[lbl]_peer_rbuild_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )

          trigger $[lbl]_global_barrier : () = \_ -> (
            ( print "Hash join done." ;
              $[next]
            ) @OnCounter(id=[# $[lbl]_join_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )
        }
}