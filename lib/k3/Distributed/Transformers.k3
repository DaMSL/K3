include "Annotation/Collection.k3"
include "Annotation/Map.k3"
include "Core/Barrier.k3"
include "Core/Builtins.k3"
include "Core/Messaging.k3"

control DistributedCollectionStatistics [ lbl         : label
                                        , query_c     : expr
                                        , coordinator : expr
                                        , nodes       : expr
                                        , next        : expr ]
{
  ?e => $[lbl]_get_cstats(); $.[e]
     +> {
          // Statistics data structure populated at the coordinator.
          // Key is bucket address, value is bucket size.
          declare $[lbl]_cstats : mut collection {key: address, value: int} @Map

          // Use this method to gather collection statistics at the coordinator.
          declare $[lbl]_get_cstats : () -> () = \_ -> (
            $[nodes].iterate (\p -> ($[lbl]_collect_cstats, p.addr) <- ())
          )

          // Worker-side trigger to respond to statistics queries.
          trigger $[lbl]_collect_cstats : () = \_ -> (
            ($[lbl]_node_cstats, $[coordinator]) <- (me, $[query_c].size())
          )

          // Coordinator-side trigger to accumulate collection statistics returns.
          // It prints the bucket distribution when all nodes are done reporting.
          trigger $[lbl]_node_cstats : (address, int) = \ns -> (
            (bind ns as (bucket_addr, bucket_size) in
              $[lbl]_cstats.insert {key: bucket_addr, value: bucket_size});
            ( print ($[|exprLabel 'lbl|] ++ " buckets: " ++
                      ($[lbl]_cstats.fold (\acc -> \r -> (acc ++ atos(r.key) ++ " => " ++ itos(r.value) ++ " ")) ""));
              $[next]
            ) @OnCounter(id=[# $[lbl]_cstats_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )
        }
}


// Distributed Group By Implementation
// Enabling profiling will display the total elapsed time for the operator
// And skew statistics, namely:
//   - Difference in time between first and last peer finished with the group-by
//   - Number of keys assigned to each peer

control DistributedGroupByGeneric [ lbl             : label
                                  , peer_next       : expr
                                  , next            : expr
                                  , merge           : expr
                                  , coordinator     : expr
                                  , nodes           : expr
                                  , profile         : expr ]
{
  ignore ( (?e.groupBy ?groupF ?aggF ?accE) : collection ?t )
     => ((if $[profile]
          then $[lbl]_start_ts = now_int ()
          else ());
        $[nodes].iterate (\p -> ($[lbl]_scan, p.addr) <- ()) )
     +> {
          declare $[lbl]_start_ts : mut int = 0
          declare $[lbl]_end_ts : mut int = 0

          trigger $[lbl]_scan : () = \_ -> (
            (ignore ( $.[e].groupBy $.[groupF] $.[aggF] $.[accE]
            ) @SendPartitionByKey( dest_trg    = [$ $[lbl]_merge]
                                 , barrier_trg = [$ $[lbl]_peer_barrier]
                                 , nodes       = nodes ))
          )

          // Note: SendByPartitionKey always uses a @Collection for the wire type.
          trigger $[lbl]_merge : collection $::[t] @Collection = \partials -> (
            partials.iterate (\w -> $[merge] w)
          )

          trigger $[lbl]_peer_barrier : () = \_ -> (
            ( $[peer_next] ;
              ( $[lbl]_global_barrier, $[coordinator] ) <- ()
            ) @OnCounter(id=[# $[lbl]_peer_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
          )

          trigger $[lbl]_global_barrier : () = \_ -> (
            ( if $[profile]
                then ( $[lbl]_end_ts = now_int ();
                       let elapsed = $[lbl]_end_ts - $[lbl]_start_ts
                       in print ($[|exprLabel 'lbl|] ++ " total time: " ++ (itos elapsed))
                     )
                else ();
              $[next]
            ) @OnCounter(id=[# $[lbl]_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ $[profile]])
          )
        }
}

control DistributedGroupBy [ lbl             : label
                           , peer_next       : expr
                           , next            : expr
                           , merge           : expr
                           , coordinator     : expr
                           , nodes           : expr
                           , profile         : expr ]
{
  ignore ( (?e.groupBy ?groupF ?aggF ?accE) : collection ?t )
     => (ignore ($.[e].groupBy $.[groupF] $.[aggF] $.[accE]))
          @DistributedGroupByGeneric(
              lbl         = lbl
            , peer_next   = [$ $[peer_next] $[lbl]_result ]
            , next        = next
            , merge       = [$ (\v ->
                                  $[lbl]_result.insert_with v (\a -> \b ->
                                    {key:a.key, value: $[merge] a.value b.value}))
                            ]
            , coordinator = coordinator
            , nodes       = nodes
            , profile     = [$ $[profile] ] )

     +> { declare $[lbl]_result : mut collection $::[t] @Map }
}


control DistributedHashJoin [ lbl             : label
                            , lhs_query       : expr
                            , rhs_query       : expr
                            , lhs_build_merge : expr
                            , rhs_probe_merge : expr
                            , peer_next       : expr
                            , next            : expr
                            , coordinator     : expr
                            , nodes           : expr
                            , lhs_build_ty    : type
                            , rhs_probe_ty    : type
                            ]
{
  () => $[nodes].iterate (\p -> ($[lbl]_lhs_scan, p.addr) <- ())
     +> {
        trigger $[lbl]_lhs_scan : () = \_ -> (
          (ignore
            ( $[lhs_query]
            ) @SendPartitionByKey( dest_trg    = [$ $[lbl]_build_lhs]
                                 , barrier_trg = [$ $[lbl]_lhs_peer_barrier]
                                 , nodes       = nodes))
        )

        trigger $[lbl]_build_lhs : $[lhs_build_ty] = \vals -> (
          vals.iterate $[lhs_build_merge]
        )

        trigger $[lbl]_lhs_peer_barrier : () = \_ -> (
          print "Hash join LHS finished build." ;
          ( $[nodes].iterate (\p -> ($[lbl]_rhs_scan, p.addr) <- ())
          ) @OnCounter(id=[# $[lbl]_peer_build_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_rhs_scan : () = \_ -> (
          (( ignore
              ( $[rhs_query]
              ) @SendPartitionByKey( dest_trg    = [$ $[lbl]_probe_rhs]
                                   , barrier_trg = [$ $[lbl]_rhs_peer_barrier]
                                   , nodes       = nodes))
          ) @OnCounter(id=[# $[lbl]_lhs_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_probe_rhs : $[rhs_probe_ty] = \vals -> (
          // Probe LHS hash table.
          vals.iterate $[rhs_probe_merge]
        )

        trigger $[lbl]_rhs_peer_barrier : () = \_ -> (
          ( print "Hash join RHS finished probe." ;
            $[peer_next] ;
            ($[lbl]_global_barrier, $[coordinator]) <- ()
          ) @OnCounter(id=[# $[lbl]_rhs_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )

        trigger $[lbl]_global_barrier : () = \_ -> (
          ( print "Hash join done." ;
            $[next]
          ) @OnCounter(id=[# $[lbl]_join_done], eq=[$ $[nodes].size()], reset=[$ false], profile=[$ false])
        )
     }
}
