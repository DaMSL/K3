include "Annotation/Collection.k3"
include "Annotation/Seq.k3"
include "Annotation/Map.k3"
include "Core/Builtins.k3"
include "Core/CppBuiltins.k3"
include "Core/Barrier.k3"
include "Core/Profile.k3"

/* K3 reimplementation of Spark ML-Lib's Naive Bayes from:
   https://github.com/apache/spark/blob/master/mllib/ \
     src/main/scala/org/apache/spark/mllib/classification/NaiveBayes.scala
*/

typedef feature_vector = collection { elem: real } @ { Vector }
typedef data_elem      = { elem: feature_vector, lbl: int }
typedef agg_elem       = { key : int, value : {count: int, sum_vector: feature_vector} }
typedef model_elem     = { key : int, value : {pi : real, theta : feature_vector} }

declare dataFiles: collection { path: string } @ { Collection }

// Generate a builtin to load data into a collection
@:CArgs 2
declare dataLoader: string -> collection data_elem @Seq -> ()

// Sequence of data points. Each point represented as a vector
declare data       :     collection data_elem  @Seq
declare aggregated :     collection agg_elem   @Map
declare model      : mut collection model_elem @Map

// Smoothing parameter
declare num_features : int  = 3
declare lambda       : real = 1.0
declare nf_times_l   : real = num_features * lambda

declare local_obs_cnt : mut int = 0
declare globl_obs_cnt : mut int = 0

declare master : address = 127.0.0.1:40000

trigger train_local : () = \_ -> (
  let peer_aggregates =
    data.groupBy (\r -> r.lbl)
                 (\acc -> (\r -> ( local_obs_cnt = local_obs_cnt + 1;
                                   {count: acc.count + 1, sum_vector: acc.sum_vector.add r.elem}
                 )))
                 {count: 0, sum_vector: zeroVector num_features}
  in
  (merge_trainings, master) <- (local_obs_cnt, peer_aggregates)
)

trigger merge_trainings : (int, collection agg_elem @Seq) = \aggs_and_count -> (
  bind aggs_and_count as (cnt, peer_aggs) in
  ( globl_obs_cnt = globl_obs_cnt + cnt;
    peer_aggs.iterate (\v ->
      case aggregated.lookup v of
       { Some a ->
           aggregated.insert
            { key: v.key,
              value: { count      : v.value.count + a.value.count,
                       sum_vector : v.value.sum_vector.add a.value.sum_vector
        } } }
       { None -> aggregated.insert v }
    );
    ( build_model ()
    ) @OnCounter(id=[# peers_merged], eq=[$ peers.size()])
  )
)

declare build_model : () -> () = \_ -> (
  let num_labels   = aggregated.size () in
  let pi_log_denom = log (globl_obs_cnt + (num_labels * lambda)) in

  model = aggregated.map (\kv ->
    let pi              = (log (kv.value.count + lambda)) - pi_log_denom in
    let fv_mag          = kv.value.sum_vector.fold (\acc -> (\r -> acc + r.elem)) 0.0 in
    let theta_log_denom = log (fv_mag + nf_times_l) in
    let theta           = kv.value.sum_vector.map (\i -> (log (i.elem + lambda)) - theta_log_denom) in
    { key: kv.key, value: {pi : pi, theta : theta} }
  )
)

trigger ready : () = \_ -> (
  ( peers.iterate (\p -> (train_local, p.addr) <- ())
  ) @OnCounter(id=[# peers_ready], eq=[$ peers.size()])
)

trigger load_all : () = \_ -> (
   dataFiles.iterate (\e -> ( print (concat "Loading File: " e.path);
                              dataLoader e.path data) );
   ((ready, master) <- ())
)

trigger shutdown: () = \_ -> haltEngine ()

source s1: () = value ()
feed s1 |> load_all