include "Annotation/Vector.k3"
include "Core/local_copy_Builtins.k3"

declare totalError : mut real 
declare outputValue : collection {elem : real } @Vector // l layers * n neurons_per_layer
declare myIndex : collection {elem : real } @Vector // l layers * n neurons_per_layer
declare gradient :  collection {elem : real } @Vector // l layers * n neurons_per_layer
//declare eta : collection {elem : real } @Vector // l layers * n neurons_per_layer
//declare alpha : collection {elem : real } @Vector // l layers * n neurons_per_layer
declare eta : mut real = 0.15
declare alpha : mut real = 0.30
declare weight : collection {elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer
declare deltaWeight : collection {elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer

declare topology : collection {elem : int} @Vector
declare inputValues : collection {elem : real } @Vector
declare targetValue: collection {elem : real } @Vector


// If the actual topology is m*n*p it is stored as (m+1)*(n+1)*p

//declare access1LayerNeuron : int -> int -> int = \layerNum -> \neuronNum -> (
//	let acc = mut neuronNum in (
//		(range (layerNum)).iterate (\i -> acc = acc + (topology.at(i.i)).elem ); 
//	acc)
//	)

//declare access2LayerNeuronOutputNeuron :  int -> int -> int -> int = \layerNum -> \neuronNum -> 
// \outNeuronNum -> (
//	let acc = mut outNeuronNum in (
//		(range (layerNum)).iterate (\i-> acc + (topology.at(i.i)).elem * (topology.at(i.i+1)).elem);
//		(range (neuronNum)).iterate (\i -> acc + (topology.at(layerNum+1)).elem );
//		acc
//	)
//) 

declare s: mut int 

declare accessPrev : int -> int = \layerNum -> (
	if (layerNum <0)
	then (0)
	else (
		if (layerNum == 0 )
		then ((topology.at(layerNum)).elem )
		else (
			(topology.at(layerNum)).elem + accessPrev (layerNum-1)
		)
	)

)

declare access1LayerNeuron : int -> int -> int = \layerNum -> \neuronNum -> (
	neuronNum + (accessPrev (layerNum-1)) 
)


declare accessPrev2 : int -> int = \layerNum -> (
	if (layerNum < 0)
	then (0)
	else (
		if (layerNum == 0 )
		then (((topology.at(layerNum)).elem) * ((topology.at(layerNum+1)).elem) )
		else (
			((topology.at(layerNum)).elem) * ((topology.at(layerNum+1)).elem) + accessPrev2 (layerNum-1)
		)
	)

)


declare access2 : {e1: int , e2: int , e3: int} -> int = \x -> bind x as {e1: layerNum , e2: neuronNum , e3 :outputNeuronNum } in  (
	let t = (accessPrev2 (layerNum-1)) in t +(topology.at((layerNum) + 1)).elem * neuronNum + outputNeuronNum
)



declare copyFirstLayer : () -> () = \_ -> (
	(range (((topology.at(0)).elem) -1)).iterate (\pos -> (
		outputValue.set (pos.i) {elem : ((inputValues.at(pos.i)).elem)}
	))
)
declare xrange : int -> int -> collection {i: int } @Seq = \start -> \end -> (
	(range end).filter (\x -> x.i >= start)
)



declare activationFunction : real -> real = \x -> (
	tanh x
)

declare activationFunctionDerivative : real -> real = \x -> (
	1 - x*x
)

declare neuronFeedForward : int -> int -> real = \layerId -> \neuronId -> (
	activationFunction (
 	let previousLayerId = (layerId-1) in 
 	((range( (topology.at( previousLayerId )).elem )).fold (\acc ->  \previousNeuronId -> (
		let a1 = access1LayerNeuron (previousLayerId) (previousNeuronId.i) in 
		(let a2 = access2 {e1: (previousLayerId), e2: (previousNeuronId.i), e3: neuronId} in 
		(acc + ((outputValue.at (a1)).elem) * ((weight.at(a2)).elem) )
		)
		
	) ) 0.0)
	
	)
	
)

declare calculateError : int -> real = \i -> (

	 let lastLayerFirstNeuronIndex = (access1LayerNeuron ((topology.at( (topology.size ()) -1 )).elem ) (0)) in  (
	 (range ( targetValue.size () )).fold (\acc -> \pos -> (  (  (outputValue.at(pos.i + lastLayerFirstNeuronIndex )).elem + acc +(targetValue.at(pos.i)).elem  )) ) 0.0 
	)
)

declare neuronIdAccess : int -> int = \index -> (
	bind (
		topology.fold(\acc -> \t -> (  
		if (acc.flag) then (if (acc.nid < t.elem ) then ({nid:acc.nid, flag:false}) else ({nid: acc.nid-t.elem, flag: true})) else ({nid: acc.nid, flag : false}) )) {nid: index, flag : true}
	) as { nid : nid, flag : flag } in (nid)



)


declare layerIdAccess : int -> int = \index -> (
	bind (
		(range(topology.size())).fold (\acc -> \i -> ( let t = ((topology.at(i.i)).elem) in  
		( if (acc.flag)  then (if (acc.num < t ) then ({num : (i.i), flag:false}) else ({num : (acc.num-t), flag : true}) ) else ({num: acc.num, flag: false}) )
		 )) {num : index, flag : true}
	
	) as { num : num, flag : flag} in (num)
)

/*
WEIRD ERROR : THIS WORKS

declare calculateError : int -> real = \i -> (

	 let x= 1.0 in  (
	 (range ( targetValue.size () )).fold (\acc -> \pos -> (  ( acc + (targetValue.at(pos.i)).elem +x )) ) 0.0 
	)
)

BUT THIS DOESN'T 

declare calculateError : int -> real = \i -> (

	 let x= 1.0 in  (
	 (range ( targetValue.size () )).fold (\acc -> \pos -> (  ( x +acc + (targetValue.at(pos.i)).elem )) ) 0.0 
	)
)
*/

declare x : mut int 
declare y : mut int 
declare r : mut real 

trigger start : () = \_ -> (
	topology.insert {elem : 4};
	topology.insert {elem : 3};
	topology.insert {elem : 4};

	//Create the input and output values
	inputValues.insert {elem : 1.0};
	inputValues.insert {elem : 0.0};
	inputValues.insert {elem : 1.0};
	
	targetValue.insert {elem : 1.0};
	targetValue.insert {elem : 0.0};
	targetValue.insert {elem : 1.0};

	
	//Create the vectors (some with random values)
	let finalLayerNum = (topology.size () -1 ) in 
	let finalNeuronNum = ((topology.at((topology.size ()) - 1)).elem -1) in (
		(range((access1LayerNeuron finalLayerNum  finalNeuronNum)+1)).iterate (\i-> ( 
			outputValue.insert {elem : (randomFraction ()) }; // set outputValue to random numbers
			gradient.insert {elem : (randomFraction ())} // set gradients to random numbers , basicaly done to create the gradient vector of the required size
		//print "Hello"
		))	
	
	);

	//Create the weights with random values 
	let secondLastLayer = (topology.size () -2 ) in 
	let lastNeuronInSecondLastLayer = ( (topology.at(secondLastLayer)).elem -1 ) in 
	let lastNeuronInLastLayer = ( (topology.at( (topology.size () - 1 ) )).elem -1 ) in 
	(
		(range (access2 {e1: secondLastLayer, e2: lastNeuronInSecondLastLayer, e3 : lastNeuronInLastLayer})).iterate (\i -> (
			weight.insert {elem : (randomFraction ()) };
			deltaWeight.insert {elem : (randomFraction ()) }	
		
		))
	);
	
	//Set ouputputValues of the bias neurons to 1
	(range (topology.size ()) ).iterate (\i -> (
	let pos = (access1LayerNeuron (i.i) ((topology.at(i.i)).elem -1) ) in
	outputValue.set pos {elem : 1.0}
	));
	
	//FEED-FORWARD
	
	// Output from the first layer is just the input layer values
	copyFirstLayer ();
	
	//feed-forward the subsequent layers
	(xrange 1 (topology.size ()) ).iterate ( \layerId -> (
		(range ((topology.at(layerId.i)).elem -1 )).iterate ( \neuronId -> ( //output of the last neuron is not modified as it is the bias neuron
			let a1 = (access1LayerNeuron layerId.i neuronId.i) in (
				outputValue.set (a1) {elem:(neuronFeedForward layerId.i neuronId.i)}		
			)	
			
		))
	
	));
	
	// try doing it with map rather than iterate
	// before that write reverse access methods i.e. given the index of outputvalue return the layerId and neuronId that it corresponds to 
	
	//BACK-PROPAGATION
	
	
	//Calculate the overall error
	
	totalError =  (
	 let lastLayerFirstNeuronIndex = (access1LayerNeuron ((topology.at( (topology.size ()) -1 )).elem ) (0)) in  (
	 (range ( targetValue.size () )).fold (\acc -> \pos -> ( let val =  ((outputValue.at(pos.i + lastLayerFirstNeuronIndex )).elem) in 
	 							let val2 = ((targetValue.at(pos.i)).elem)  in (  
	 				 
	 				   (acc - val) + val2  )) ) 0.0 
	)
	);
	
	totalError = sqrt (totalError/  ( targetValue.size () ));
	
	
	//calculate the output layer gradients

	let outputLayerId = (topology.size()-1) in 
	let numNeuronsOutLayer = (topology.at(outputLayerId)).elem in 
	( (range numNeuronsOutLayer).iterate ( \neuronId -> (
		let index = (access1LayerNeuron outputLayerId neuronId.i) in 
		gradient.set index {elem : 
				(  ((targetValue.at(neuronId.i)).elem - (outputValue.at(index)).elem ) * (activationFunctionDerivative (outputValue.at(index)).elem)  ) }	
	
	)) 
	);

	//calculate the hidden layer gradients
	let size = topology.size() in 
	((xrange 2 (topology.size ()) ).iterate (\i -> (
	  let presentLayerId = (  (topology.size ()) - i.i) in
	  let numNeuronsPresentLayer = ((topology.at(presentLayerId)).elem) in 
	  let numNeuronsNextLayer = ((topology.at(presentLayerId+1)).elem )in 
	  (
	    (range(numNeuronsPresentLayer)).iterate(\neuronId -> (
	      let presentNeuronIndex = (access1LayerNeuron presentLayerId neuronId.i) in
	      let nextLayerIndices = (range (outputValue.size ())).filter (\index -> ((layerIdAccess index.i) ==(presentLayerId+1) )) in
	      (
	        gradient.set 
	        presentNeuronIndex 
	        {elem : (nextLayerIndices.fold (\acc -> \nextLayerNeuronIndex -> 
	              ( let nextLayerNeuronId = (neuronIdAccess nextLayerNeuronIndex.i) in
	                let index2 = (access2 {e1:presentLayerId, e2:neuronId.i, e3: nextLayerNeuronId} ) in  
	                ( ((weight.at(index2)).elem) * ((gradient.at(nextLayerNeuronIndex.i)).elem) ) 
	              ) ) 0.0 ) * (activationFunctionDerivative (outputValue.at(presentNeuronIndex)).elem )
	        } 
	    
	      )
	    ))
	  
	  )
	
	)));
	
	
	//Update the connection weights (output -> first hidden layer)
	(xrange 1 (topology.size ())).iterate (\i -> (
	  let presentLayerId = ((topology.size ()) - i.i) in
	  let previousLayerId = (presentLayerId - 1) in 
	  let numNeuronsPresentLayer = ((topology.at(presentLayerId)).elem) in 
	  (
	    (range numNeuronsPresentLayer).iterate (\presentLayerNeuronId -> ( 
	      let previousLayerNeuronIndices =  (range (outputValue.size ())).filter (\index -> ((layerIdAccess index.i) ==(previousLayerId) )) in
	      (
	        previousLayerNeuronIndices.iterate(\previousLayerIndex -> ( 
	          let previousLayerNeuronId = (neuronIdAccess previousLayerIndex.i) in 
	          let previousLayerNeuronIndex2 = (access2 {e1: previousLayerId, e2: previousLayerNeuronId, e3: presentLayerNeuronId.i}) in 
	          (
	            deltaWeight.set previousLayerNeuronIndex2 {elem : ( ((gradient.at(previousLayerIndex.i)).elem * eta) + ( (deltaWeight.at(previousLayerNeuronIndex2)).elem *alpha) )};
	            weight.set previousLayerNeuronIndex2 {elem : ((weight.at(previousLayerNeuronIndex2)).elem + (deltaWeight.at(previousLayerNeuronIndex2)).elem )}
	          ) 
	        
	        ))
	      )	  
	    ))
	  )	
	));
	print "Hello"
)

source s1 : () = value ()
feed s1 |> start
