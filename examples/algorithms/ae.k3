include "Annotation/Vector.k3"
include "Core/Builtins.k3"

//typedefs
typedef Connection = {weight :  mut real , deltaWeight : mut real }
typedef Neuron = {m_outputValue: mut real , m_myIndex: mut int, m_gradient: mut real , m_outputWeights : mut collection { connection : Connection } @Seq, eta: real, alpha : real } 
typedef Layer =  collection {neuron : mut  Neuron } @Seq

//typedef testInt = 
typedef Net = {m_error : mut real , m_layer : mut collection {layer : Layer} @Seq, m_test : collection {elem : int} @Seq}

//declarations
declare net : mut Net
declare newNeuron : mut Neuron
declare newConnection : mut Connection

declare newLayer : mut Layer
declare nextLayer : mut Layer


declare numOutputs : mut int

declare inputValues :  mut collection {i : mut real} @ {Seq}
declare outputValues : mut collection {i : mut real} @ {Seq}
declare resultValues : collection {i : mut real} @ {Seq}

declare numLayers :  mut int 

declare topology : mut collection {i: int} @ {Seq}


declare peers_seq : mut collection {addr: address} @ {Seq}
declare sum : mut real 
declare dow : mut real 

declare i : mut int
declare testLayer : mut Layer 
declare testNeuron : mut Neuron 
declare testInt : mut int
//declare testReal  : real 


declare master : address = 127.0.0.1:40000
declare outputLayer : mut Layer 
// functions

declare updateWeights : Neuron -> Layer -> ()  = \neuron -> \prevLayer -> (
	
	
)

declare sumDOW : Neuron -> Layer -> real =  \neuron -> \nextLayer -> (
	// TODO : Add the first part of product as well, m_outputWeights[i].weight 
	//nextLayer.fold (\sum -> \elem -> sum + elem.neuron.m_gradient  ) 0.0
	1.0
)


declare calculateOutputGradients : Neuron -> real -> real = \neuron -> \targetValue -> (
	(neuron.m_outputValue - targetValue) * (activationFunctionDerivate neuron.m_outputValue)
)

declare calculateHiddenGradients : Neuron -> Layer -> real = \neuron -> \nextLayer -> (
	(sumDOW neuron nextLayer) * (activationFunctionDerivate neuron.m_outputValue)	
)
//Activation Function 
//TODO : Add tanh functin in Builtins.k3 and the backend

declare activationFunction : real -> real =  \x -> (
	1.0	// Modify this
)
declare activationFunctionDerivate : real -> real = \x -> (
	1.0 //Modify this
)

//This is equivalent to Neuron feedForward 
declare neuronFeedForward : Net -> int -> int -> Layer -> real = \net-> \layerNum-> \neuronIndex -> \prevLayer -> (
	// Change these to use fold function
	//fold    : (a -> (content -> a)) -> a -> a
	//sumMyC = myCollection.fold (\acc -> (\r -> acc + r.x)) 0;
	//sum = prevLayer.fold (\sum -> (\elem -> sum + elem.neuron.m_outputValue * (elem.neuron.m_outputWeights.at(neuronIndex)).connection.weight )) 0.0;

	
	sum = 0.0;		
	//(range (prevLayer.size ())).iterate (\elem -> ( sum = sum+ (prevLayer.at(elem.i)).neuron.m_outputValue * ((prevLayer.at(elem.i)).neuron.m_outputWeights.at(neuronIndex)).connection.weight  )); 

	// TODO 
	//testNeuron = ((net.m_layer.at(layerNum)).layer.at(neuronIndex)).neuron;
	//testNeuron.m_outputValue = sum;
		
	//((net.m_layer.at(layerNum)).layer.at(neuronIndex)).neuron.m_outputValue = sum;

	//print "Done";
	activationFunction sum
)

declare createNeuron : int -> int -> Neuron = \numOutputs -> \myIndex -> (
	let newNeuron = {m_outputValue: mut 0.0 , m_myIndex: mut myIndex, m_gradient: mut 0.0 , m_outputWeights : empty {connection : Connection} @Seq , eta: 0.15, alpha : 0.15 } in 
	(
	(range numOutputs).iterate (
		\conn -> (
			((newNeuron.m_outputWeights).insert {connection : {weight: mut 0.0, deltaWeight: mut 0.0}}); // Update to random number 
			print "Done"
		)
	);
	newNeuron )
)

declare backProp : Net -> collection {i: mut real } @Seq -> () = \net -> \outputValues -> (

	// calculate overall net error
	//i = net.m_layer.size () - 1;
	//outputLayer = net.m_layer.at()
	outputLayer = (net.m_layer.at((net.m_layer.size ())-1)).layer;
	//net.m_error = 0.0;
	// delta = outputValues[i] - outputLayer[i].getOutputValue();
	//outputLayer.


	// calculate output layer gradients

	// calculate hidden layer gradients

	// Update the connection weights (output -> hidden layer) 

	print "Done"	

)



declare xrange : int -> int -> collection {i: int } @Seq  = \start -> \end -> (
	(range end).filter (\x -> x.i >= start)
)

declare netFeedForward : Net ->  Net  = \net -> (

	// Output from the first layer is just the input layer values
	(range (inputValues.size ())).iterate( 
		\i -> (
			bind (((net.m_layer.at(0)).layer.at(i.i)).neuron ) as { m_outputValue : x} in ( x = (inputValues.at(i.i)).i)
		)
	); 
        
        //feedforward the subsequent layers
        //TODO ..need to start this from 1 rather than 0

        (xrange 1 (net.m_layer.size ())).iterate (
                \layerNum -> (
                        let prevLayer = (net.m_layer.at(layerNum.i -1)).layer in 
                        (xrange 0 (((net.m_layer.at(layerNum.i)).layer).size ()-1)).iterate (\i -> (
                                //((net.m_layer.at(layerNum.i)).layer).at()
                                sum = neuronFeedForward net layerNum.i i.i prevLayer
                        ) )
                )

        );
        
	
	// Perform this update in functional format as well, might have to do it layerwise though
	//print "Done with FeedForward"
	net
)


declare inputValuesList :  mut collection { iv : collection {i : mut real} @ {Seq}} @Seq
declare outputValuesList : mut collection { iv : collection {i : mut real} @ {Seq}} @Seq

declare fileName : string
declare readDcdData : string -> int -> collection { iv : collection {i : mut real} @ {Seq}} @Seq = \fileName -> \maxData -> (
	inputValuesList
)

// triggers
declare newNet : mut Net
declare updateNetwork : Net -> Net = \oldNet -> (
	newNet 
)
declare peerSize : mut int 
declare peersResponded : mut int = 0 

trigger proceedAsMaster : () = \_ -> (
	peerSize = peers.size ();
	if (peersResponded == peerSize) then () else ();
	if (peersResponded == peerSize) then (
		// iterate over the peers and update their network with the updated net at master 
		print "Updating the slave networks"
	) 
	else (
		//Update the net at master with the net received from the slave

		peersResponded = peersResponded+1 
	);
	
	print "Proceeding as Master"
)

trigger proceedAsSlave : () = \_ -> (

	print "Proceeding as Slave";

	(range 1000).iterate ( 
		\_  ->  (

		inputValuesList = readDcdData fileName 1000;
		outputValuesList = inputValuesList;

		inputValuesList.iterate (
			\i -> (
				inputValues = i.iv; 
				net = netFeedForward net ; 
				outputValues = inputValues; 
				backProp net outputValues
			)
		);	
		
		(proceedAsMaster, me) <- ();	
		print "Done with one set of reading "
		)
	);

	print "I was a slave"
)


trigger configureNetwork : () = \_ -> (
	print "Configuring the network ";
	topology.insert {i: 5};
	topology.insert {i: 10};
	topology.insert {i: 5};

	numLayers = topology.size ();
	//net.m_error = 0.0;

	let net = {m_error: mut 0.0, m_layer: empty {layer: Layer } @Seq } in 
	(
	(range numLayers).iterate (
		\layerNum -> (
			if numLayers-1 == layerNum.i then (numOutputs = 0) else (numOutputs = (topology.at(layerNum.i + 1)).i );

			(range ((topology.at(layerNum.i)).i + 1) ).iterate (
				\neuronNum -> (
					((net.m_layer.at(layerNum.i)).layer).insert {neuron : (createNeuron numOutputs neuronNum.i)}
					//if neuronNum.i == (topology.at(layerNum.i)).i then (((net.m_layer.at(layerNum.i)).layer.at(neuronNum.i)).neuron.m_outputValue = 1.0 ) else () 
				)  
			)
		)
	)
	);
	if master == me then ((proceedAsMaster, me) <- ()) else ((proceedAsSlave, me) <- ())
)

trigger startSlave : () = \_ -> (
	peers.iterate (\i -> peers_seq.insert i);
	(configureNetwork, me) <- ();
	
	print "Done"
)


trigger startServer : () = \_ -> (

	// In genral, the toplogy will be read from a congfiguration file or the yaml file.
	// Hence makes sense to create the network configuration along with other data sources

	(configureNetwork, me) <- ();
	print "Parameter Server started"
)


source server : () = value ()
feed server |> startServer

source slave : () = value ()
feed slave |> startSlave
