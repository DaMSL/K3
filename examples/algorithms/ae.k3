include "Annotation/Vector.k3"
include "Core/Builtins.k3"

//typedefs
typedef Connection = {weight :  mut real , deltaWeight : mut real }
typedef Neuron = {m_outputValue: mut real , m_myIndex: mut int, m_gradient: mut real , m_outputWeights : mut collection { connection : Connection } @Seq, eta: real, alpha : real } 
typedef Layer =  collection {neuron : mut  Neuron } @Seq
typedef Net = {m_error : mut real , m_layer : mut collection {layer : Layer} @Seq}

//declarations
declare net : mut Net
declare newNeuron : mut Neuron
declare newConnection : mut Connection

declare newLayer : mut Layer
declare nextLayer : mut Layer


declare numOutputs : mut int

declare inputValues :  mut collection {i : mut real} @ {Seq}
declare outputValues : mut collection {i : mut real} @ {Seq}
declare resultValues : collection {i : mut real} @ {Seq}

declare numLayers :  mut int 

declare topology : mut collection {i: int} @ {Seq}


declare peers_seq : mut collection {addr: address} @ {Seq}
declare sum : mut real 
declare dow : mut real 

declare i : mut int
declare testLayer : mut Layer 
declare testNeuron : mut Neuron 
declare testInt : mut int
//declare testReal  : real 


declare master : address = 127.0.0.1:40000
declare outputLayer : mut Layer 
// functions

declare xrange : int -> int -> collection {i: int } @Seq  = \start -> \end -> (
	(range end).filter (\x -> x.i >= start)
)

declare updateWeights : Neuron -> Layer -> collection { connection : Connection } @Seq  = \neuron -> \prevLayer -> (
	empty  { connection : Connection } @Seq
)

declare sumDOW : Neuron -> Layer -> real =  \neuron -> \nextLayer -> (
	/*
		double sum = 0.0;
	
	for (unsigned i=0; i< nextLayer.size()-1; i++){
		sum +=  m_outputWeights[i].weight * nextLayer[i].m_gradient;
	
	}
	
	return sum;
	
	


	(range (nextLayer.size () -1)).iterate (\i -> ( 
			bind (neuron.m_outputputWeights.at(i.i)) as {weight : w } in (
			
			
			)		
	));*/
	// TODO : Add the first part of product as well, m_outputWeights[i].weight 
	//nextLayer.fold (\sum -> \elem -> sum + elem.neuron.m_gradient  ) 0.0
	1.0
)


declare calculateOutputGradients : Neuron -> real -> real = \neuron -> \targetValue -> (
	(neuron.m_outputValue - targetValue) * (activationFunctionDerivate neuron.m_outputValue)
)

declare calculateHiddenGradients : Neuron -> Layer -> real = \neuron -> \nextLayer -> (
	(sumDOW neuron nextLayer) * (activationFunctionDerivate neuron.m_outputValue)	
)
//Activation Function 
declare activationFunction : real -> real =  \x -> (
	tanh x	
)
declare activationFunctionDerivate : real -> real = \x -> (
	1.0 - x*x 
)


declare createNeuron : int -> int -> Neuron = \numOutputs -> \myIndex -> (
	let newNeuron = {m_outputValue: mut 0.0 , m_myIndex: mut myIndex, m_gradient: mut 0.0 , m_outputWeights : empty {connection : Connection} @Seq , eta: 0.15, alpha : 0.15 } in 
	(
	(range numOutputs).iterate (
		\conn -> (
			((newNeuron.m_outputWeights).insert {connection : {weight: mut 0.0, deltaWeight: mut 0.0}}); // TODO : Update to random number 
			print "Done"
		)
	);
	newNeuron )
)

declare backProp : Net -> collection {i: mut real } @Seq -> Net = \net -> \outputValues -> (
	
	// calculate overall net error
	//let outputLayer = (net.m_layer.at(net.m_layer.size () -1)).layer in
	/*
	bind ((net.m_layer.at(net.m_layer.size () -1))) as {layer : outputLayer} in 
	(
		
		(range (outputLayer.size () -1 )).iterate (\i-> ( let delta = (outputValues.at(i.i)).i - (outputLayer.at(i.i)).neuron.m_outputValue in 
			bind net as {m_error : x } in ( x = net.m_error + delta*delta)
		));
		
		//bind net as {m_error : x } in (x = net.m_error/(outputLayer.size () -1 ) );
		//bind net as {m_error : x } in (x = sqrt 1.0);	 
		print "Hello"	
	);
	*/

	// calculate output layer gradients
	bind ((net.m_layer.at(net.m_layer.size () -1))) as {layer : outputLayer} in (
		(range (outputLayer.size () -1 )).iterate (\i-> ( bind ((outputLayer.at(i.i)).neuron) as {m_gradient : x} in 
			(x =  calculateOutputGradients (outputLayer.at(i.i)).neuron (outputValues.at(i.i)).i) ))
	);
	
	// calculate hidden layer gradients
	//TODO : Reverse this
	(xrange 1 (net.m_layer.size()-1) ).iterate (\layerNum -> (
		bind ((net.m_layer.at(layerNum.i))) as {layer : hiddenLayer} in (
			bind ((net.m_layer.at(layerNum.i+1))) as {layer : nextLayer} in (
				(range (hiddenLayer.size())).iterate (\i-> ( 
					bind ((hiddenLayer.at(i.i)).neuron) as {m_gradient : x} in 
						(x =  calculateHiddenGradients (outputLayer.at(i.i)).neuron nextLayer)								
				))
			)
		)
	));
	/*
	//Update the connection weights (output -> first hidden layer) 
	
	for (unsigned layerNum = m_layers.size()-1 ; layerNum > 0; layerNum--){
		Layer &layer = m_layers[layerNum];
		Layer &prevLayer = m_layers[layerNum-1];
		
		for (unsigned i=0; i< layer.size()-1; i++ ){
			layer[i].updateWeights(prevLayer);
		}
	} 
	
	*/
	// Update the connection weights (output -> hidden layer) 
	//TODO : Reverse this 
	/*
	(xrange 1 (net.m_layer.size())).iterate (\layerNum -> (
		bind ((net.m_layer.at(layerNum.i))) as {layer : presentLayer} in (
			bind ((net.m_layer.at(layerNum.i-1))) as {layer : prevLayer} in (				
				(range (presentLayer.size()-1)).iterate (\i-> ( print "hello"	))
			)
		)	
	
	));
	*/
	net

)

//This is equivalent to Neuron feedForward 
declare neuronFeedForward : Net -> int -> int -> Layer -> real = \net-> \layerNum-> \neuronIndex -> \prevLayer -> (	
	//fold    : (a -> (content -> a)) -> a -> a
	let sum = prevLayer.fold (\sum -> (\elem -> sum + elem.neuron.m_outputValue * (elem.neuron.m_outputWeights.at(neuronIndex)).connection.weight )) 0.0 in 
	activationFunction sum
)


//declare reverse collection {i: int } @Seq -> collection {i: int } @Seq = 

declare netFeedForward : Net ->  Net  = \net -> (

	// Output from the first layer is just the input layer values
	(range (inputValues.size ())).iterate( 
		\i -> (
			bind (((net.m_layer.at(0)).layer.at(i.i)).neuron ) as { m_outputValue : x} in ( x = (inputValues.at(i.i)).i)
		)
	); 
	
    
        
        //feedforward the subsequent layers
        (xrange 1 (net.m_layer.size ())).iterate (
                \layerNum -> (
                        let prevLayer = (net.m_layer.at(layerNum.i -1)).layer in 
                        (xrange 0 (((net.m_layer.at(layerNum.i)).layer).size ()-1)).iterate (\i -> (
                                let sum = neuronFeedForward net layerNum.i i.i prevLayer in 
                                bind (((net.m_layer.at(layerNum.i)).layer.at(i.i)).neuron ) as {m_outputValue : x} in (x= sum)
                        ) )
                )
        );
        
    	// Fixing the output of the bias term in all the layers to 1.0
        (xrange 0 (net.m_layer.size ())).iterate (
                \layerNum -> (
                        let prevLayer = (net.m_layer.at(layerNum.i -1)).layer in 
                        (xrange 0 (((net.m_layer.at(layerNum.i)).layer).size ()-1)).iterate (\i -> (
                                let lastElementPosition = (((net.m_layer.at(layerNum.i)).layer).size ()-1) in 
                                bind (((net.m_layer.at(layerNum.i)).layer.at(lastElementPosition)).neuron ) as {m_outputValue : x} in (x= 1.0)
                        ) )
                )
        );
	net
)


declare inputValuesList :  mut collection { iv : collection {i : mut real} @ {Seq}} @Seq
declare outputValuesList : mut collection { iv : collection {i : mut real} @ {Seq}} @Seq

declare fileName : string
declare readDcdData : string -> int -> collection { iv : collection {i : mut real} @ {Seq}} @Seq = \fileName -> \maxData -> (
	inputValuesList
)

// triggers
declare newNet : mut Net
declare updateNetwork : Net -> Net = \oldNet -> (
	newNet 
)
declare peerSize : mut int 
declare peersResponded : mut int = 0 

trigger proceedAsMaster : () = \_ -> (
	peerSize = peers.size ();
	if (peersResponded == peerSize) then () else ();
	if (peersResponded == peerSize) then (
		// iterate over the peers and update their network with the updated net at master 
		print "Updating the slave networks"
	) 
	else (
		//Update the net at master with the net received from the slave

		peersResponded = peersResponded+1 
	);
	
	print "Proceeding as Master"
)

trigger proceedAsSlave : () = \_ -> (

	print "Proceeding as Slave";

	(range 1000).iterate ( 
		\_  ->  (

		inputValuesList = readDcdData fileName 1000;
		outputValuesList = inputValuesList;

		inputValuesList.iterate (
			\i -> (
				inputValues = i.iv; 
				net = netFeedForward net ; 
				outputValues = inputValues; 
				net = backProp net outputValues
			)
		);	
		
		(proceedAsMaster, me) <- ();	
		print "Done with one set of reading "
		)
	);

	print "I was a slave"
)


trigger configureNetwork : () = \_ -> (
	print "Configuring the network ";
	topology.insert {i: 5};
	topology.insert {i: 10};
	topology.insert {i: 5};

	numLayers = topology.size ();

	TODO : change this to 
	let net = {m_error: mut 0.0, m_layer: empty {layer: Layer } @Seq } in 
	(
	(range numLayers).iterate (
		\layerNum -> (
			if numLayers-1 == layerNum.i then (numOutputs = 0) else (numOutputs = (topology.at(layerNum.i + 1)).i );

			(range ((topology.at(layerNum.i)).i + 1) ).iterate (
				\neuronNum -> (
					((net.m_layer.at(layerNum.i)).layer).insert {neuron : (createNeuron numOutputs neuronNum.i)}
					//if neuronNum.i == (topology.at(layerNum.i)).i then (((net.m_layer.at(layerNum.i)).layer.at(neuronNum.i)).neuron.m_outputValue = 1.0 ) else () 
				)  
			)
		)
	)
	);
	if master == me then ((proceedAsMaster, me) <- ()) else ((proceedAsSlave, me) <- ())
)

trigger startSlave : () = \_ -> (
	peers.iterate (\i -> peers_seq.insert i);
	(configureNetwork, me) <- ();
	
	print "Done"
)


trigger startServer : () = \_ -> (

	// In genral, the toplogy will be read from a congfiguration file or the yaml file.
	// Hence makes sense to create the network configuration along with other data sources

	(configureNetwork, me) <- ();
	print "Parameter Server started"
)


source server : () = value ()
feed server |> startServer

source slave : () = value ()
feed slave |> startSlave
