include "Annotation/Vector.k3"
include "Core/Builtins.k3"

declare master: address = 127.0.0.1:40000

declare totalError : mut real 
declare topology : collection { elem : int} @Vector

declare inputValues : mut collection { elem : real } @Vector
declare targetValue : mut collection { elem : real } @Vector
declare outputValue : collection { elem : real } @Vector // l layers * n neurons_per_layer
declare zValue      : collection { elem : real } @Vector // l layers * n neurons_per_layer
declare deltaValue  : collection { elem : real } @Vector // l layers * n neurons_per_layer
declare gradient    : collection { elem : real } @Vector // l layers * n neurons_per_layer

//declare eta         : mut real = 0.50
declare alpha       : mut real = 0.30
declare lambda      : mut real = 0.20

declare weight      : collection { elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer
declare deltaWeight : collection { elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer

declare weightUpdateBroadcast : mut collection { elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer
declare weightsLastUpdated    : mut collection { elem : real } @Vector // l layers * n neurons_per_layer * n neurons_per_next_layer

declare row_number           : mut int = -1
declare epochSize            : int = 8
declare peerMessagesReceived : mut int  

declare s : mut int
declare rows : mut int = 0

declare allData : collection { elem : {i: real , j : real , k : real} } @Vector

// Access all neurons for previous layer
declare accessPrev : int -> int = \layerNum -> (
	if (layerNum < 0)
		then 0
	else (
		if (layerNum == 0)
			then ((topology.at layerNum).elem )
		else (
			(topology.at layerNum).elem + accessPrev (layerNum - 1)
		)
	)
)

declare accessPrev2 : int -> int = \layerNum -> (
	if (layerNum < 0)
		then 0
	else (
		if (layerNum == 0)
			then (((topology.at(layerNum)).elem) * ((topology.at(layerNum+1)).elem))
		else (
			((topology.at(layerNum)).elem) * ((topology.at(layerNum+1)).elem) + accessPrev2 (layerNum - 1)
		)
	)
)

// Compute z-value and delta value for each neuron
declare access1LayerNeuron : int -> int -> int = \layerNum -> \neuronNum -> (
	neuronNum + (accessPrev (layerNum - 1)) 
)

// Calculate weights for each neuron using input neuron with corresponding layer and output neuron(access2 -> accessWeigths)
declare access2 : {e1: int , e2: int , e3: int} -> int = \x -> 
	bind x as {e1: layerNum , e2: neuronNum , e3 :outputNeuronNum } in  (
		let t = (accessPrev2 (layerNum - 1)) in
			t + ((topology.at(layerNum + 1)).elem * neuronNum) + outputNeuronNum
)

// Make all inputs and outputs equal to each other
declare copyFirstLayer : () -> () = \_ -> (
	(range (((topology.at(0)).elem) - 1)).iterate (\pos -> (
		(outputValue.set (pos.i) { elem : ((inputValues.at(pos.i)).elem) });
		(zValue.set (pos.i) { elem : ((inputValues.at(pos.i)).elem) })
	))
)

declare xrange : int -> int -> collection { i: int } @Seq = \start -> \end -> (
	(range end).filter (\x -> x.i >= start)
)

// Activation Function for network. Can also use sigmoid function
declare activationFunction : real -> real = \x -> (
	tanh x
)

declare activationFunctionDerivative : real -> real = \x -> (
	1 - x*x
)

// 
declare neuronCalculateZValue : int -> int -> real = \layerId -> \neuronId -> (
 	let previousLayerId = (layerId - 1) in (
 	(range((topology.at previousLayerId).elem)).fold(\acc -> \previousNeuronId -> (
		let a1 = access1LayerNeuron (previousLayerId) (previousNeuronId.i) in (
		let a2 = access2 {e1: (previousLayerId), e2: (previousNeuronId.i), e3: neuronId} in (
			acc + ((outputValue.at a1).elem) * ((weight.at a2).elem)
			)
		)
	)) 0.0)
)

declare neuronFeedForward : int -> int -> real = \layerId -> \neuronId -> (
	activationFunction (neuronCalculateZValue (layerId) (neuronId))
)

declare calculateError : int -> real = \i -> (
	let lastLayerFirstNeuronIndex = (access1LayerNeuron ((topology.at( (topology.size ()) -1 )).elem ) (0)) in  (
	(range ( targetValue.size () )).fold(\acc -> \pos -> (
		((outputValue.at(pos.i + lastLayerFirstNeuronIndex)).elem + acc + (targetValue.at(pos.i)).elem))
	) 0.0 )
)

declare neuronIdAccess : int -> int = \index -> (
	bind (topology.fold(\acc -> \t -> (
		if (acc.flag)
			then (
				if (acc.nid < t.elem)
					then ({nid:acc.nid, flag:false})
				else 
					({nid: acc.nid-t.elem, flag: true})
			)
		else 
			({nid: acc.nid, flag : false})
		)
	) {nid: index, flag : true}) as { nid : nid, flag : flag } in (nid)
)

declare layerIdAccess : int -> int = \index -> (
	bind ((range(topology.size())).fold(\acc -> \i -> (
		let t = ((topology.at(i.i)).elem) in (
			if (acc.flag)
				then (
					if (acc.num < t)
						then ({num : (i.i), flag: false})
					else (
						{num : (acc.num-t), flag: true})
					)
			else ({num: acc.num, flag: false}))
		 )) {num : index, flag: true}
	) as { num : num, flag: flag} in (num)
)

// Set up number of neurons to each layer(input, hidden, output)
declare initializeMaster : () -> () = \_ -> (
	//Architecture of the network
	topology.insert {elem : 3};
	topology.insert {elem : 4};
	topology.insert {elem : 2};

	//Create the weights with random values 
	let secondLastLayer = (topology.size () -2 ) in 
	let lastNeuronInSecondLastLayer = ( (topology.at(secondLastLayer)).elem -1 ) in 
	let lastNeuronInLastLayer = ( (topology.at( (topology.size () - 1 ) )).elem -1  ) in 
	(
		(range (access2 {e1: secondLastLayer, e2: lastNeuronInSecondLastLayer, e3 : lastNeuronInLastLayer} +1 )).iterate (\i -> (
			weight.insert { elem : (randomFraction ()) };
			deltaWeight.insert { elem : 0.0 };
			weightsLastUpdated.insert { elem : 0.0 };
			weightUpdateBroadcast.insert { elem : 0.0 }	
		))
	)
)

// Give required values to each neurons
declare initialize : () -> () = \_ -> (
	//Architecture of the network
	(initializeMaster ());

	//Create the vectors (some with random values)
	let finalLayerNum = (topology.size () - 1 ) in 
	let finalNeuronNum = ((topology.at((topology.size ()) - 1)).elem - 1) in (
		(range((access1LayerNeuron finalLayerNum  finalNeuronNum) + 1)).iterate (\i-> ( 
			outputValue.insert {elem : (randomFraction ()) }; // set outputValue to random numbers
			deltaValue.insert {elem : (randomFraction ()) }; // set outputValue to random numbers    
			zValue.insert {elem : (randomFraction ()) }; // set outputValue to random numbers    
			gradient.insert {elem : (randomFraction ())} // set gradients to random numbers, basically done to create the gradient vector of the required size
		))	
	);	
	
	//Set ouputputValues of the bias neurons to 1
	(range (topology.size ())).iterate (\i -> (
		let pos = (access1LayerNeuron (i.i) ((topology.at(i.i)).elem -1) ) in
		outputValue.set pos {elem : 1.0}
	))
)

declare setDeltaWeightsTozero : () -> () = \_ -> (
  // Update the weights
  (range ((topology.size ()) - 1)).iterate(\layerId -> (
    let numPresentLayerNeurons = (topology.at layerId.i).elem in
    let numNextLayerNeurons = (topology.at (layerId.i + 1)).elem in (
    	(range (numPresentLayerNeurons)).iterate(\presentLayerNeuronId -> (
    	(range (numNextLayerNeurons - 1)).iterate(\nextLayerNeuronId -> (
    	let index2Weight = access2 {e1 : layerId.i, e2 : presentLayerNeuronId.i, e3 : nextLayerNeuronId.i } in (
    		weight.set index2Weight {elem : 0.0})
        ))
      ))
    )
  ))
)

// Feed Forward method to train neural network
declare slaveFeedForward : () -> () = \_ ->(
	
	// Output from the first layer is just the input layer values
	copyFirstLayer ();
	
	//feed-forward the subsequent layers
	(xrange 1 (topology.size ()) ).iterate ( \layerId -> (
		(range ((topology.at(layerId.i)).elem -1 )).iterate ( \neuronId -> ( 
			//output of the last neuron is not modified as it is the bias neuron
			let a1 = (access1LayerNeuron layerId.i neuronId.i) in (
				(outputValue.set (a1) {elem:(neuronFeedForward layerId.i neuronId.i)});		
				(zValue.set (a1) {elem:(neuronCalculateZValue layerId.i neuronId.i)})		
			)	
		))
	))
)

// Back Propagation algorithm for gradient descent
declare slaveBackPropagation : () -> () = \_ -> (

	//BACK-PROPAGATION
	//Calculate the overall error
	totalError =  (
	let lastLayerFirstNeuronIndex = (access1LayerNeuron ((topology.at( (topology.size ()) -1 )).elem ) (0)) in  (
	let r = range ( targetValue.size () ) in
	r.fold (\acc -> \pos -> (
		let val = ((outputValue.at((pos.i+ lastLayerFirstNeuronIndex))).elem) in
		let val2 = ((targetValue.at(pos.i)).elem) in (
			((0.0 - val ) + val2)*((0.0 - val ) + val2) + acc)
	 	)) 0.0 )
	);
	
	totalError = sqrt (totalError / ((targetValue.size ()) - 1));

	//calculate the output layer deltas 
	let outputLayerId = (topology.size() - 1) in 
	let numNeuronsOutLayer = (topology.at outputLayerId).elem in (
		(range numNeuronsOutLayer).iterate(\neuronId -> (
		let index = (access1LayerNeuron outputLayerId neuronId.i) in 
			deltaValue.set index {elem : (((outputValue.at index).elem - (targetValue.at neuronId.i).elem) * (activationFunctionDerivative (zValue.at index).elem))
		))
	);

	//calculate the hidden layer deltas
	((xrange 2 (topology.size ()+1) ).iterate (\i -> (
	let presentLayerId = ((topology.size ()) - i.i) in
	let numNeuronsPresentLayer = ((topology.at presentLayerId).elem) in 
	let numNeuronsNextLayer = ((topology.at (presentLayerId+1)).elem) in (
		(range numNeuronsPresentLayer).iterate(\neuronId -> (
		let presentLayerNeuronIndex = (access1LayerNeuron presentLayerId neuronId.i) in
		let nextLayerIndices = (range (outputValue.size ())).filter (\index -> ((layerIdAccess index.i) == (presentLayerId+1) )) in (
			deltaValue.set 
	        presentLayerNeuronIndex {elem : let productWeightDelta = (nextLayerIndices.fold (\acc -> \nextLayerNeuronIndex -> (
	        	let nextLayerNeuronId = (neuronIdAccess nextLayerNeuronIndex.i) in
	        	let index2 = (access2 {e1:presentLayerId, e2:neuronId.i, e3: nextLayerNeuronId} ) in ( 
	        	)) 0.0 ) in ( 
	        		productWeightDelta * (activationFunctionDerivative (zValue.at presentLayerNeuronIndex).elem )
	        	)}
	        )
	    ))
	)
	)));

	// Calculate the delta weights 
	(range ((topology.size ()) -1)).iterate(\layerId -> (
    let numPresentLayerNeurons = (topology.at layerId.i).elem in
    let numNextLayerNeurons = (topology.at (layerId.i + 1)).elem in (
    (range (numPresentLayerNeurons)).iterate(\presentLayerNeuronId -> (
    (range (numNextLayerNeurons ) ).iterate(\nextLayerNeuronId -> (
    	let index2DeltaWeight = access2 {e1 : layerId.i, e2 : presentLayerNeuronId.i, e3 : nextLayerNeuronId.i } in
    	let index1DeltaValue = (access1LayerNeuron (layerId.i +1) (nextLayerNeuronId.i)) in
    	let index1OutputValue = (access1LayerNeuron (layerId.i) (presentLayerNeuronId.i)) in (
            deltaWeight.set
            index2DeltaWeight 
            {elem : (deltaWeight.at index2DeltaWeight).elem + ((outputValue.at index1OutputValue).elem * (deltaValue.at index1DeltaValue).elem)}
        )
        ))
    ))
    )));

    // Update the weights
    (range ((topology.size ()) -1)).iterate(\layerId -> (
    let numPresentLayerNeurons = (topology.at layerId.i).elem in
    let numNextLayerNeurons = (topology.at (layerId.i + 1)).elem in (
    (range numPresentLayerNeurons).iterate(\presentLayerNeuronId -> (
    (range numNextLayerNeurons).iterate(\nextLayerNeuronId -> (
    let index2Weight = access2 {e1 : layerId.i, e2 : presentLayerNeuronId.i, e3 : nextLayerNeuronId.i } in (
    	weight.set index2Weight {elem : (weight.at index2Weight).elem - alpha * (((deltaWeight.at index2Weight).elem)/(allData.size ()) + (
    		if ((topology.at layerId.i).elem != (presentLayerNeuronId.i + 1))
    			then (lambda*((weight.at index2Weight).elem))
    		else (0)
    	))} 
    )))
    ))
    )
  ))
)

trigger startMaster : () = \_ -> (
	initializeMaster ()
)

trigger receiveAtMaster : collection {elem : real } @Vector = \updateOnWeights -> (
	(weight.inPlaceAdd (updateOnWeights.sub weightUpdateBroadcast));
	if (peerMessagesReceived == peers.size())
		then (
			(weightUpdateBroadcast = weight.sub weightsLastUpdated);
			peers.iterate(\peer -> (
			(receiveAtSlave, peer.addr) <- (weightUpdateBroadcast)));
			(weightsLastUpdated = weight);
			peerMessagesReceived = 0
		)
	else (
		peerMessagesReceived = peerMessagesReceived + 1
	)
)

trigger receiveAtSlave : collection {elem : real } @Vector = \updateOnWeights -> (
	weight.inPlaceAdd (updateOnWeights)
)

trigger startSlave : () = \_ -> (
	(range 20).iterate (\r -> (	
	allData.iterate( \csvData -> (
    inputValues = empty {elem : real} @Vector;

	//Create the input and output values
	inputValues.insert {elem : csvData.elem.i};
	inputValues.insert {elem : csvData.elem.j};
	inputValues.insert {elem : 1.0};
	
    targetValue = empty {elem : real} @Vector;
    targetValue.insert {elem : csvData.elem.k};
    targetValue.insert {elem : 1.0};

    (if (row_number == -1) 
    	then (initialize ()) 
    else ()
    );
    row_number = row_number + 1;

    print (itos row_number);		
    /*
		(if (row_number % epochSize == 0) 
		then (
			(receiveAtMaster, master) <- (weight.sub weightsLastUpdated);
			weightsLastUpdated = weight
		) 
		else () );
    */
	(slaveFeedForward ());
	(slaveBackPropagation ());
    
	//(results_sink, me) <- { addr : me, iter : r.i , totalError : totalError}
    print "Done with an iteration of ff and bp"
	))
	))
)

trigger storeData :  {i: real, j: real, k : real} = \csvData -> (
	allData.insert {elem : {i: csvData.i, j: csvData.j, k: csvData.k} };
	rows = rows +1;
	(if (rows==8) 
		then ((startSlave, me) <- ()) 
	else ())
)

//sink results_sink : real = stdout csv
//sink results_sink : {addr : address,   iter: int , totalError : real } = stdout csv

source masterSource : () = value ()
feed masterSource |> startMaster 

source slaveSource1 :  {i: real , j: real , k : real}  = file "f1.csv" csv 
feed slaveSource1 |> storeData

